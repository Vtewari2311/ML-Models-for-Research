{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PolynomialRegressionCompute.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-7dGc_FWaxN"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style.use('ggplot')"
      ],
      "metadata": {
        "id": "Y0XdFpgCWklj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolynomialRegression(object):\n",
        "        \"\"\"PolynomialRegression \n",
        "        \n",
        "        Parameters\n",
        "        ------------\n",
        "        x_pts : 1-d numpy array, shape = [n_samples,]\n",
        "        y_pts : 1-d numpy array, shape = [n_samples,]\n",
        "    \n",
        "        \n",
        "        Attributes\n",
        "        ------------\n",
        "        theta : 1-d numpy array, shape = [polynomial order + 1,] \n",
        "            Ceofficients of fitted polynomial, with theta[0] corresponding\n",
        "            to the intercept term        \n",
        "        \n",
        "        method : str , values = 'normal_equation' | 'gradient_descent'\n",
        "            Method used for finding optimal values of theta\n",
        "        \n",
        "        If gradient descent method is chosen:\n",
        "        \n",
        "            costs : 1-d numpy array,\n",
        "                Cost function values for every iteration of gradient descent\n",
        "            \n",
        "            numIters: int\n",
        "                Number of iterations of gradient descent to be performed\n",
        "            \n",
        "        References\n",
        "        ------------\n",
        "        https://en.wikipedia.org/wiki/Polynomial_regression\n",
        "        \"\"\"\n",
        "\n",
        "    def __init__(self, x, y):     \n",
        "        \n",
        "        self.x = x\n",
        "        self.y = y      \n",
        "    \n",
        "    def standardize(self,data):\n",
        "        \"\"\" Peform feature scaling\n",
        "        Parameters:\n",
        "        ------------\n",
        "        data : numpy-array, shape = [n_samples,]\n",
        "        \n",
        "        Returns:\n",
        "        ---------\n",
        "        Standardized data                  \n",
        "        \"\"\"\n",
        "\n",
        "        return (data - np.mean(data))/(np.max(data) - np.min(data))\n",
        "        \n",
        "    def hypothesis(self, theta, x):\n",
        "        \"\"\" Compute hypothesis, h, where\n",
        "        h(x) = theta_0*(x_1**0) + theta_1*(x_1**1) + ...+ theta_n*(x_1 ** n)\n",
        "        Parameters:\n",
        "        ------------\n",
        "        theta : numpy-array, shape = [polynomial order + 1,]        \n",
        "        x : numpy-array, shape = [n_samples,]\n",
        "        \n",
        "        Returns:\n",
        "        ---------\n",
        "        h(x) given theta values and the training data\n",
        "        \"\"\"       \n",
        "        h = theta[0]\n",
        "        for i in np.arange(1, len(theta)):\n",
        "            h += theta[i]*x ** i        \n",
        "        return h        \n",
        "        \n",
        "    def computeCost(self, x, y, theta):\n",
        "        \"\"\" Compute value of cost function J \n",
        "        \n",
        "        Parameters:\n",
        "        ------------\n",
        "        x : numpy array, shape = [n_samples,]\n",
        "        y : numpy array, shape = [n_samples,]\n",
        "        \n",
        "        Returns:\n",
        "        ---------\n",
        "        Value of cost function J at value theta given the training data\n",
        "        \n",
        "        \"\"\"    \n",
        "        m = len(y)  \n",
        "        h = self.hypothesis(theta, x)\n",
        "        errors = h-y\n",
        "        \n",
        "        return (1/(2*m))*np.sum(errors**2) \n",
        "        \n",
        "    def fit(self, method = 'normal_equation', order = 1, tol = 10**-3, numIters = 20, learningRate = 0.01):\n",
        "        \n",
        "        \"\"\"Fit theta to the training data\n",
        "        \n",
        "        Parameters\n",
        "        -----------\n",
        "        method: string, values = 'normal_equation' | 'gradient_descent'\n",
        "             Indicates method for which polynomial regression will be performed\n",
        "            \n",
        "        order: int, optional\n",
        "             Order of polynomial fit. Defaults to 1 (linear fit)\n",
        "             \n",
        "        numIters: int, optional\n",
        "             Number of iterations of gradient descent to be performed\n",
        "            \n",
        "        learningRate: float, optional\n",
        "             \n",
        "        tol : float, optional\n",
        "            Value indicating the cost value (J(theta)) at which\n",
        "            gradient descent should terminated. Defaults to 10 ** -3\n",
        "            \n",
        "        Returns:\n",
        "        -----------\n",
        "        self : object\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        if method == 'normal_equation': \n",
        "            d = {}\n",
        "            d['x' + str(0)] = np.ones([1,len(x_pts)])[0]    \n",
        "            for i in np.arange(1, order+1):                \n",
        "                d['x' + str(i)] = self.x ** (i)        \n",
        "                \n",
        "            d = OrderedDict(sorted(d.items(), key=lambda t: t[0]))\n",
        "            X = np.column_stack(d.values())  \n",
        "\n",
        "            theta = np.matmul(np.matmul(linalg.pinv(np.matmul(np.transpose(X),X)), np.transpose(X)), self.y)\n",
        "\n",
        "        elif method == 'gradient_descent':\n",
        "                \n",
        "            d = {}\n",
        "            d['x' + str(0)] = np.ones([1,len(x_pts)])[0]    \n",
        "            for i in np.arange(1, order+1):                \n",
        "                d['x' + str(i)] = self.standardize(self.x ** (i))      \n",
        "                \n",
        "            d = OrderedDict(sorted(d.items(), key=lambda t: t[0]))\n",
        "            X = np.column_stack(d.values())  \n",
        "                \n",
        "            m = len(self.x)\n",
        "            theta = np.zeros(order + 1)           \n",
        "            costs = []\n",
        "            for i in range(numIters):\n",
        "             \n",
        "                h = self.hypothesis(theta, self.x)       \n",
        "                errors = h-self.y\n",
        "                theta += -learningRate * (1/m)*np.dot(errors, X)\n",
        "                cost = self.computeCost(self.x, self.y, theta)\n",
        "                costs.append(cost)         \n",
        "                #tolerance check\n",
        "                if cost < tol:\n",
        "                    break\n",
        "                \n",
        "            self.costs = costs\n",
        "            self.numIters = numIters\n",
        "            \n",
        "        self.method = method    \n",
        "        self.theta = theta        \n",
        "\n",
        "        return self\n",
        "        \n",
        "    def plot_predictedPolyLine(self):\n",
        "        \"\"\"Plot predicted polynomial line using values of theta found\n",
        "        using normal equation or gradient descent method\n",
        "        \n",
        "        Returns\n",
        "        -----------       \n",
        "        matploblib figure\n",
        "        \"\"\"        \n",
        "        plt.figure()\n",
        "        plt.scatter(self.x, self.y, s = 30, c = 'b') \n",
        "        line = self.theta[0] #y-intercept \n",
        "        label_holder = []\n",
        "        label_holder.append('%.*f' % (2, self.theta[0]))\n",
        "        for i in np.arange(1, len(self.theta)):            \n",
        "            line += self.theta[i] * self.x ** i \n",
        "            label_holder.append(' + ' +'%.*f' % (2, self.theta[i]) + r'$x^' + str(i) + '$') \n",
        "\n",
        "        plt.plot(self.x, line, label = ''.join(label_holder))        \n",
        "        plt.title('Polynomial Fit: Order ' + str(len(self.theta)-1))\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y') \n",
        "        plt.legend(loc = 'best')      \n",
        "\n",
        "    def plotCost(self):\n",
        "        \"\"\"Plot number of gradient descent iterations verus cost function, J,\n",
        "        values at values of theta\n",
        "        \n",
        "        Returns\n",
        "        -----------       \n",
        "        matploblib figure\n",
        "        \"\"\"        \n",
        "        if self.method == 'gradient_descent':\n",
        "            plt.figure()\n",
        "            plt.plot(np.arange(1, self.numIters+1), self.costs, label = r'$J(\\theta)$')\n",
        "            plt.xlabel('Iterations')\n",
        "            plt.ylabel(r'$J(\\theta)$')\n",
        "            plt.title('Cost vs Iterations of Gradient Descent')\n",
        "            plt.legend(loc = 'best')\n",
        "        else:\n",
        "            print('plotCost method can only be called when using gradient descent method')\n"
      ],
      "metadata": {
        "id": "Xqmvh6o1WoCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}